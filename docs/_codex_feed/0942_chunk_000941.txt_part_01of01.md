# File: chunk_000941.txt (part 1 of 1)
```
creation and deployment process chain without a human-in-the-loop An important concept in the use of AI systems is ensuring that there is a ‘human-in-the-loop’. This means that there should always be a human who is involved and responsible for the oversight of an end-to-end process. AI systems may be used to perform the individual actions to create, test, and deploy software – but the entirety of the creation and deployment pipeline should not be fully automated. It is also important to understand that different AI systems may be trained or tuned to specific aspects of development. An AI system that is used to create software may not be specifically trained to create secure software. PCI DSS Requirement 6 covers the security of software development and remains a good reference for use of AI systems. Provided with access to systems and information not required for their operation PCI DSS Requirement 7 notes the importance of ‘least privilege’ (providing only the minimum level of privileges needed to perform a job) and ‘need to know’ (providing access to only the least amount of data needed to perform a job). These items hold true when considering the use of AI systems. It can be tempting to give an AI system access to as many systems and as much information as possible, but this increases the potential impact if things go wrong or work in a way that was not expected. AI Systems Should Be: Provided with access to account data only when it is suitably protected AI systems have been used in payments to help detect fraud and manage risk in payments for a long time. Relatively new, however, is the use of agentic AI to facilitate and even make payments on behalf of cardholders. These operations obviously require access to some form of payment card data; however, this does not mean that the data cannot be protected. In a later principle, the use of single-use PANs and payment tokens is discussed as a potential way to align the need for AI to use payment data, whilst also keeping that payment data secure. Requirement 3 of PCI DSS covers how to secure cardholder data at rest, and Requirement 4 covers securing this data during transmission, and these requirements apply equally to AI-based systems. Deployed so that the actions performed by the AI can be logged and monitored, and a (human) individual held responsible for those actions In one of the previous principles the importance of understanding an AI cannot be held responsible was noted. This principle is another aspect of that – there needs to be ways to log and monitor what an AI system is doing so that the actions can be traced back to the system performing that action, and a human individual can be held responsible for those actions. The role of the responsible individual is important here, as AI systems can often involve large teams, but ultimate responsibility must come down to a single individual. Where possible, logging should be sufficient to audit the prompt inputs and reasoning process used by the AI system that led to the output provided. PCI DSS Requirement 10 covers the need for logging and monitoring and should be considered in this context. Validated prior to, and throughout, deployment to confirm they continue to work as expected The need for validation is important across the entire lifecycle of an AI system; from training, to initial deployment, and on-going validation of the correctness of operation. This includes the consideration for supply chain risk – where does the AI model come from, where does the training data come from, etc. When deploying and using AI systems it’s important to understand that they are often inherently non-deterministic – the output obtained from a system may change given the exact same input. Additionally, systems can ‘drift’ over time as they obtain and process more input (which may include malicious input from threat actors). This makes the need for on-going validation even more important. PCI DSS Requirement 6 and 11 should be considered. Implemented so that they can be easily disabled if required An important question to ask when implementing any system is, “what if something goes wrong?” This is equally true for AI systems, and so as part of planning any AI deployment, ensure there is a clear and well-understood process for disabling operation. This may seem obvious, but it can sometimes be complex if not designed in from the outset. Protected against malicious input and malformed output When securing software and data-repository interfaces, the need to protect against malicious input is well understood. A similar type of security is equally important for AI systems, which can be vulnerable to attacks like ‘prompt-injection’ and ‘data-poisoning’. Fundamentally these types of attacks exploit an unsecured interface to the AI system, with the goal to get the AI to perform privileged actions, reveal sensitive data, or change its behaviour in a way that is useful to the malicious entity. In a payment system, AI systems should be protected against attacks aiming to perform fraudulent payments. Therefore, placing controls over the types of input or commands the AI system can receive and act upon, and filtering the output of the AI system, are important. Provided with limited, use case, and context specific credentials for any required access A previous principle outlined that AI systems should not be ‘provided with access to systems and information not
```

