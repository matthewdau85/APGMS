# File: chunk_000940.txt (part 1 of 1)
```
Artificial intelligence (AI) systems are increasingly being used within businesses to help in the creation, management, and operation of payment systems and environments. Their use is expanding beyond systems directly managed by humans, to agentic AI systems, which have a level of agency to perform actions on their own behalf. The rapid pace of change in AI systems means it can be difficult to understand the potential risk posed by these systems, and how to best securely deploy their use. In collaboration with the payment security industry, the PCI Security Standards Council (PCI SSC) offers high-level principles to consider when developing and deploying AI systems. The principles are expressed in terms of things that should not be , things that should be , and things that may be – as they relate to the use of AI systems. Although this content is guidance only, the list also starts with a single item that must be . Note that the following list is not intended to be exhaustive. As this industry is rapidly innovating, PCI SSC will continue to evolve and adapt and evaluate these principles on a regular basis to ensure industry alignment. Overview of AI Principles Explore AI Principles in More Detail: AI Systems Must Be: AI systems must be deployed and managed in compliance with applicable PCI SSC requirements Use of AI does not remove or bypass the need to meet the requirements of any applicable PCI SSC standard. For example, if your implementation is in scope for PCI DSS, the AI systems need to be implemented in accordance with PCI DSS requirements. This includes how data is secured as it is stored, processed, and transmitted. This can sometimes be complex, and AI systems can often operate in ways that are not easily decomposed and understood. However, this complexity does not remove the need to meet any applicable requirement. The following principles are designed to help with understanding the security nuances of AI systems, and some may call out specific PCI DSS requirements. Any individual requirement references should not be interpreted as indicating that only those requirements apply, but as examples of requirements that are likely to be impacted or require consideration in light of that principle. AI Systems Should Not Be: Trusted with high-impact secrets or unprotected sensitive data AI systems have been known to provide output that contains sensitive information, either without direct prompting or through specifically and maliciously formatted prompt input. Additionally, the data processing and flow of AI systems may not be fully contained or constrained within an entity’s control. Preventing this exposure of sensitive data starts by limiting the sensitive data provided to the AI system in the first place. This includes ensuring that data used to train the AI system is sanitised of sensitive information and secrets prior to use, as well as ensuring production data used and trusted to these systems is appropriately secured. Examples of sensitive data that should be kept out of AI datasets include API access tokens and user credentials (that are not specific to that AI), unsecured account data, and cryptographic keys or key material. Given agency to perform operations which require the formal acceptance of responsibility It is important to remember that even with the increasing ability of AI, these systems are not human individuals, and they cannot accept or take on responsibility. Therefore, operations and roles which require the formal acceptance of responsibility are not suitable for AI systems. This includes roles such as key custodians (a role which is also prevented by the ‘no high-impact secrets’ principle above) and providing management-level authorization or approvals. Used to generate security-sensitive random or secret values One situation where AI can be said to be just as good as humans is in generating random numbers and other unique, secret values. That is, both AI and humans are just as bad at ‘thinking up’ random values! It is often a challenge to determine if a value is truly random or not, and the recommendation is always to use a known-good random number generator (RNG), in a way that will keep the values that are generated secret and safe. It may seem reasonable to have an AI system interface to such a known-good RNG where random values are required, but if the use is generating high-impact secrets (cryptographic keys, passwords, etc.), the use will violate the first ‘should not’ principle anyway. Of course, if the random values are not security sensitive, this principle may not apply. Implemented with full agency over the creation and deployment process chain without a human-in-the-loop An important concept in the use of AI systems is ensuring that there is a ‘human-in-the-loop’. This means that there should always be a human who is involved and responsible for the oversight of an end-to-end process. AI systems may be used to perform the individual actions to create, test, and deploy software – but the entirety of the creation and deployment pipeline should not be fully automated. It is also important to understand that different AI systems may be trained or tuned to specific aspects of development. An AI system that is used to create software may not be specifically trained to create secure software. PCI DSS Requirement 6 covers the security of software development and remains a good reference for use of AI systems. Provided with access to systems and information not required for their operation PCI DSS Requirement 7
```

