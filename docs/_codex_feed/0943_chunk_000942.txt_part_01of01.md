# File: chunk_000942.txt (part 1 of 1)
```
and data-repository interfaces, the need to protect against malicious input is well understood. A similar type of security is equally important for AI systems, which can be vulnerable to attacks like ‘prompt-injection’ and ‘data-poisoning’. Fundamentally these types of attacks exploit an unsecured interface to the AI system, with the goal to get the AI to perform privileged actions, reveal sensitive data, or change its behaviour in a way that is useful to the malicious entity. In a payment system, AI systems should be protected against attacks aiming to perform fraudulent payments. Therefore, placing controls over the types of input or commands the AI system can receive and act upon, and filtering the output of the AI system, are important. Provided with limited, use case, and context specific credentials for any required access A previous principle outlined that AI systems should not be ‘provided with access to systems and information not required for their operation’. The other side of that is ensuring that any access that is provided is appropriately limited and specific to the needs and implementation of that AI system. AI systems should be provided with their own credentials, which can be easily tracked and revoked if necessary (as per the principle for disabling systems if required). Where possible, these credentials should be tied to the AI system in some way that mitigates the risk of credential theft and reuse by a malicious party (for example, through use of bound credentials). Any credentials provided should meet the requirements of least privilege and need to know. Another aspect of secure AI systems is considering which human users (or other automated systems) should be provided with access to the AI system. Considered as a potential ‘malicious insider’ during threat analysis exercises and incident response walk-throughs PCI DSS Requirement 12 covers information security policy, processes, and incident response. Specifically, PCI DSS Requirement 12.10 covers the need for an incident response plan. These plans should cover the potential for malicious AI use, either by external parties subverting the normal AI operation or misuse of AI systems by internal staff. Where an AI is given some agency to perform actions within the network or environment, incident response plans should consider the potential for the AI to become a ‘malicious insider’ itself. Implemented in a way that secures the operational environment, context / user-specific data from other users and other AI systems. PCI DSS Requirement 1 covers the importance of implementing secure networks, and although a segmented network is not required, segmentation and isolation of AI systems can help reduce the risk of these systems being used in a malicious context, or exposing sensitive systems, data, or functionality though unexpected operation. Segmentation and security controls around AI systems may also be implemented at the system level – at the level of the physical machine, virtual machine, OS, container, etc. AI Systems May Be: Provided with access to protected payment data When implementing AI systems that access cardholder data, consider the use of payment tokens or single-use PANs to limit the scope and impact of the AI system. Although, beyond the scope of PCI requirements, implementation of limits on the use of these values (limited total spend or frequency of spend, limited to use by a single merchant, limited usable lifetime, etc) can further reduce risk. Where access does not strictly require the use of the full PAN, truncated PANs or PANs encrypted whilst still maintaining cleartext BIN data, may be suitable to mitigate any risk associated with AI use. Used to provide input to an approval decision and perform actions after an approval decision has been made Although an AI system cannot assume responsibility, it is very reasonable to use AI systems to provide input to an authorization decision. For example, AI systems may be useful in determining and implementing patches for networked systems. A responsible human would be expected to authorize such use – however, based on an appropriate risk analysis this may extend from a blanket-level approval through to specific approval for each individual system to be patched. PCI DSS Requirement 6 covers change management, including the need for auditability and approval. These requirements apply equally to AI-based systems. Trusted to autonomously perform fail-secure actions AI systems can monitor many different sources of information and perform some actions more quickly than a human. Although a previous principle noted the importance of authorization for AI actions, one area where action prior to authorization may be appropriate is in response to on-going attacks. Here, an AI system may be trusted to perform proactive isolation, network throttling, or other mitigations without direct human approval. However, care must be taken if such control is provided to an AI system: Can this lead to more easily deployed denial of service attacks? What access and permissions do the AI need to perform these actions, and how can that access and permission structure be misused? Used to gather, review, and summarize content Probably one of the most common uses of AI currently is to parse, and provide output base on, large sets of data. Log reviews, as required by PCI DSS Requirement 10.4, are a good example of this. However, as previous principles have noted, it remains important to continually check and validate that these systems are providing the correct and expected output – a worst-case scenario for this type of implementation is where the AI system keeps providing summarised logs that
```

