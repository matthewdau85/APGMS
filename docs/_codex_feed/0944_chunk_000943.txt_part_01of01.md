# File: chunk_000943.txt (part 1 of 1)
```
may be trusted to perform proactive isolation, network throttling, or other mitigations without direct human approval. However, care must be taken if such control is provided to an AI system: Can this lead to more easily deployed denial of service attacks? What access and permissions do the AI need to perform these actions, and how can that access and permission structure be misused? Used to gather, review, and summarize content Probably one of the most common uses of AI currently is to parse, and provide output base on, large sets of data. Log reviews, as required by PCI DSS Requirement 10.4, are a good example of this. However, as previous principles have noted, it remains important to continually check and validate that these systems are providing the correct and expected output – a worst-case scenario for this type of implementation is where the AI system keeps providing summarised logs that show everything is OK, when there are in fact areas which need more attention. Used to generate content, as part of a product development or deployment process AI is now often used in the generation of content – written documents (such as policy documents) and software are the most common forms of content which may be produced in this way. Although the importance of a ‘human-in-the-loop’ was previously noted, this does not mean that AI systems cannot be used in content generation and system/software testing. Used as part of user-interaction systems Finally, the use of AI systems for user-interaction is both common and increasing. This is a valid use-case, if appropriate controls are put in place. It is reasonable to expect that any AI system which is exposed to public access will be subject to all types of attempted manipulation, from the silly (getting a helpdesk chatbot to help write code) to the damaging (manipulating a chatbot to expose sensitive data or details on other users).
```

